# -*- mode: ruby -*-
# vi: set ft=ruby :
### SET THIS TO -test (DASH TEST) TO ENABLE TEST MODE ###
TEST=""
INTERACTIVE = true
LIFECYCLE = "dev#{TEST}"
DOMAIN = "anytown.usa"

MIRROR_SSH_PORT_ADD = 0; MIRROR_FORWARD_PORT_ADD=0;
PURPOSE = "#{File.basename(Dir.getwd)}#{TEST}"
if PURPOSE.include? "mirror"; MIRROR_SSH_PORT_ADD += 10000; MIRROR_FORWARD_PORT_ADD=1; end
HOSTNAME_SUFFIX = "#{PURPOSE}.#{LIFECYCLE}.#{DOMAIN}"

POD_CT = File.read("./POD_CT").to_i
MSTR_CT = File.read("./MSTR_CT").to_i
CTLR_CT = File.read("./CTLR_CT").to_i
CONTROLLER = "controller"
MASTER = "master"
NODE = "node"

ROUTE="192.168.0.0"
ROUTE_MASK = 16
POD_NETWORK = "#{ROUTE}/#{ROUTE_MASK}"

NODE_NETWORK="192.168"
NODE_MASK = 16
mstr_ip_pool = ["#{NODE_NETWORK}.0.20"]
mstr_nm_pool = ["#{MASTER}"]
ctlr_ip_pool = []#["#{NODE_NETWORK}.10"]
ctlr_nm_pool = []#["#{CONTROLLER}"]
pod_ip_pool = []
pod_nm_pool = []

IMAGE = "ubuntu/bionic64"
PROVIDER = "virtualbox"
ROOT_KEY_SCRIPT = "rootkey.sh"
VAGRANT_KEY_SCRIPT = "vagrantkey.sh"

NODE_SCRIPT = "node#{TEST}.sh"
MASTER_SCRIPT = "master#{TEST}.sh"
CONTROLLER_SCRIPT = "controller#{TEST}.sh"

VM_NAME_base = "#{PURPOSE}_#{LIFECYCLE}-"
VM_NAME_CONTROLLER = "#{PURPOSE}_#{LIFECYCLE}-#{CONTROLLER}"
VM_NAME_MASTER = "#{PURPOSE}_#{LIFECYCLE}-#{MASTER}"
VM_NAME_NODE = "#{PURPOSE}_#{LIFECYCLE}-#{NODE}"

VAGRANTFILE_API_VERSION = "2"
### START Vagrant.configure ###
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

IPLIST = ""
NAMELIST = ""
FQDN = ""
ETC_HOSTS = ""

for i in 1..POD_CT  do; pod_ip_pool.append("#{NODE_NETWORK}.0.#{i+100}"); pod_nm_pool.append("#{NODE}#{i}"); end

host_ip_pool = ctlr_ip_pool + mstr_ip_pool
host_nm_pool = ctlr_nm_pool + mstr_nm_pool

for i in (0..POD_CT - 1) do
   "#{host_ip_pool.append(pod_ip_pool[i])}"
   "#{host_nm_pool.append(pod_nm_pool[i])}"
end

CONTROLLERS=""
PODS=""
MASTERS=""
host_ip_pool.each do |ip| IPLIST+="#{ip} " end
for i in (0..CTLR_CT+MSTR_CT+POD_CT - 1) do
   NAMELIST += "#{host_nm_pool[i]} "
   FQDN += "#{host_nm_pool[i]}.#{PURPOSE}.#{LIFECYCLE}.#{DOMAIN} "
   ETC_HOSTS += "\n#{host_ip_pool[i]} #{host_nm_pool[i]}.#{PURPOSE}.#{LIFECYCLE}.#{DOMAIN} #{host_nm_pool[i]}"

   if "#{host_nm_pool[i]}".include? "#{NODE}"; PODS += "#{host_ip_pool[i]} " ; end
   if "#{host_nm_pool[i]}".include? "#{CONTROLLER}"; CONTROLLERS += "#{host_ip_pool[i]} " ; end
   if "#{host_nm_pool[i]}".include? "#{MASTER}"; MASTERS += "#{host_ip_pool[i]} " ; end
end

# WRITES:   /usr/local/bin/ssh-*
SSHPODNODES="podnodes"
SSHALL="sshall"
SSHMASTER="sshmaster"
SSHRESETNODES="resetnodes"
SSHJOINNODES="joinallnodes"
JOINNODE="joinnode"
$BIN_RUNSSH = <<EOF
DIR="/usr/local/bin"
script_list=()

FNAME="#{SSHPODNODES}"
script_list+=("${FNAME}")
echo -e "#!/bin/bash
### #{SSHPODNODES} ####
if [ \\$\# -gt 0 ]; then for ip in #{PODS}; do ssh \\${ip} \\"\\${*}\\"; done;
else echo -e \\" \\$(basename \\${0}) Usage: Need argument to send\\"; fi
" > ${DIR}/${FNAME}

FNAME=#{SSHALL}
script_list+=("${FNAME}")
echo -e "#!/bin/bash
### #{SSHALL} ####
if [ \\$\# -gt 0 ]; then for ip in #{IPLIST}; do ssh \\${ip} \\"\\${*}\\"; done;
else echo -e \\" \\$(basename \\${0}) Usage: Need argument to send\\"; fi
" > ${DIR}/${FNAME}

FNAME=#{SSHMASTER}
script_list+=("${FNAME}")
echo -e "#!/bin/bash
### #{SSHMASTER} ####
if [ \\$\# -gt 0 ]; then for ip in #{MASTERS}; do ssh \\${ip} \\"\\${*}\\"; done;
else echo -e \\" \\$(basename \\${0}) Usage: Need argument to send\\"; fi
" > ${DIR}/${FNAME}

FNAME=#{SSHRESETNODES}
script_list+=("${FNAME}")
echo -e "#!/bin/bash
### #{SSHRESETNODES} ####
if [ \\$\# -eq 0 ]; then for ip in #{PODS}; do echo Resetting node @ \\${ip} && echo && ssh \\${ip} \\"sudo su - -c 'kubeadm reset -f && rm -rf /joincluster'\\"; done
else echo \\" Usage: Expect 0 args\\"; fi
" > ${DIR}/${FNAME}

FNAME=#{SSHJOINNODES}
script_list+=("${FNAME}")
echo -e "#!/bin/bash
### #{SSHJOINNODES} ####
if [ \\$\# -eq 0 ]; then for ip in #{PODS}; do ssh \\${ip} \\"echo && hostname -f && ./joincluster\\"; done
else echo \\" Usage: Expect 0 args\\"; fi
" > ${DIR}/${FNAME}

FNAME=#{JOINNODE}
script_list+=("${FNAME}")
echo -e "#!/bin/bash
### #{JOINNODE} ####
if [ \\$\# -gt 0 ]; then for node in \\$\@; do ssh \\${node} \\"echo && hostname -f && ./joincluster\\"; done
else echo \\" Usage: Expect node name arg(s)\\"; fi
" > ${DIR}/${FNAME}


################################################
###
### SET PERMISSIONS
###
################################################
for script in ${script_list[@]}; do
   chown vagrant:vagrant ${DIR}/${script}
   chmod 700 ${DIR}/${script}
done
################################################


EOF

# APPENDS:  /etc/hosts
$ETC_HOSTS = <<EOF
x=`grep -n "127.0.2.1" /etc/hosts | cut -f1 -d:` \
   && y=`cat /etc/hosts|wc -l` \
   && z=`expr $y - $x` \
   && cp -p /etc/hosts /etc/hosts.bak \
   && cat>/etc/hosts<<<`head -n -${z} /etc/hosts.bak`
echo -e '#{ETC_HOSTS}' >> /etc/hosts
EOF

# WRITES:   /etc/init/synchosts.sh
$ETC_INIT_SYNCHOSTS = <<EOF
echo #{MSG} Running: \\$ETC_INIT_SYNCHOSTS
if [ ! -d /etc/init ]; then mkdir /etc/init && chmod 755 ${_}; fi
echo '#!/bin/bash
> /root/.ssh/known_hosts && chmod 600 /root/.ssh/known_hosts
for ip in #{IPLIST}; do
   ssh-keyscan -H ${ip} >> /root/.ssh/known_hosts; done
for nm in #{NAMELIST}; do
   ssh-keyscan -H ${nm} >> /root/.ssh/known_hosts; done
for nm in #{FQDN}; do
   ssh-keyscan -H ${nm} >> /root/.ssh/known_hosts; done
cp -f /root/.ssh/known_hosts /home/vagrant/.ssh/known_hosts \
   && chown vagrant:vagrant /home/vagrant/.ssh/known_hosts \
   && chmod 600 /home/vagrant/.ssh/known_hosts
' > /etc/init/synchosts.sh \
&& chmod 755 /etc/init/synchosts.sh
EOF

# WRITES:   /etc/init.d/ntpsync.sh
# LINKS:    /etc/rc3.d/S01ntpsync
$ETC_INITD_NTPSYNC = <<EOF
echo #{MSG} Running: \\$ETC_INITD_NTPSYNC
echo '#!/bin/bash

if ! ntpstat > /dev/null 2>&1
   then service ntp stop && ntpdate time.nist.gov && service ntp start
   ntp_tries=6 && ntp_delay_seconds=8 && i=0
   while ! ntpstat
     do sleep ${ntp_delay_seconds} && i=`expr ${i} + 1`
     if [ ${i} -ge ${ntp_tries} ]
       then yellow "NTP:" && echo bailing && break
     fi
   done
fi' > /etc/init.d/ntpsync.sh \
   && chmod 755 /etc/init.d/ntpsync.sh \
   && ln -sfn /etc/init.d/ntpsync.sh /etc/rc3.d/S01ntpsync
EOF

$SYNCTEST_MASTER_ONLY = <<EOF
echo #{MSG} Running: \\$SYNCTEST_MASTER_ONLY
echo -e "#!/bin/bash\n#{SSHPODNODES} 's=\\\`hostname -f\\\` && echo -n \\"\`hostname\` to \\\${s} -> Success\\" && s=\\" and \\\`hostname\\\` back for -> Success\\" && ssh master \\"echo -n \\\\\\"\\\${s}\\\\\\" && hostname -f\\"'
" > /root/test-node-connections \
   && chown vagrant:vagrant /root/test-node-connections \
   && chmod 744 /root/test-node-connections \
   && ln -fn /root/test-node-connections /home/vagrant/test-node-connections \
   && /root/test-node-connections

EOF

$APPEND_ETC_BASHRC = <<EOF
echo #{MSG} Running: \\$APPEND_ETC_BASHRC
bashrc=shrc
library=/etc/k.library

echo '### START BASHRC' >> ${bashrc}
x=`grep -n "### START BASHRC" ${bashrc} | head -1 | cut -f1 -d:` \
   && y=`cat ${bashrc}|wc -l` \
   && z=`expr $y - $x` \
   && cp -p ${bashrc} ${bashrc}.bak \
   && cat>${bashrc}<<<`head -n -${z} ${bashrc}.bak`
echo export HISTFILE=/vagrant/bash_history  >> ${bashrc}
echo "echo -e \\"\\n### Login: \\`date\\` ###\\" >> \\${HISTFILE}"  >> ${bashrc}
#{BASHRC} >> ${bashrc}
#{BASHLIBRARY} > ${library}
#{KLIBRARY} >> ${library}
EOF

$GENKEY = <<EOF
ssh-keygen -t rsa -b 4096 -f "/root/.ssh/provisioned_id_rsa.`openssl rand -hex 12`" -q -N ""
ssh-keygen -t rsa -b 4096 -f "/home/vagrant/.ssh/provisioned_id_rsa.`openssl rand -hex 12`" -q -N ""
EOF

### WRITE /ETC/HOSTS VERY EARLY
$WRITE_HOSTS_APTUPDATE_PYTHON = <<EOF
sed -i 's/mesg\\ n\\ ||\\ true/test\\ -t\\ 0\\ \\&\\&\\ mesg\\ n/' /root/.profile
echo -e '#{ETC_HOSTS}' >> /etc/hosts \
   && echo -e 'added to /etc/hosts: #{ETC_HOSTS}'
apt-get -qq update
#{PYTHON}
EOF

$WRITE_SYNC = $ETC_INIT_SYNCHOSTS + $BIN_RUNSSH + <<-EOF
   date >> /etc/vagrant_provisioned_at
EOF

$ADHOC = ""

###
### MASTER ###
###

# $CREATE_CLUSTER
####################################################
# =>        writes file: *
# =>        Changes owner to vagrant
# =>        Hard links to ~vagrant
####################################################
      $CREATE_CLUSTER = <<-EOF
script_list=()


WORDPRESS_MYSQL_APP_SCRIPT=wordpress_mysql_app_deploy
script_list+=("${WORDPRESS_MYSQL_APP_SCRIPT}")
echo -e "############ write ${WORDPRESS_MYSQL_APP_SCRIPT} ############"
echo '#!/bin/bash
source /etc/k.library
########################
###
### CREATES & DEPLOYS wordpress_mysql IN NAMESPACE
###
###  -n namespace NAMESPACE ${NAMESPACE}
###  -o nodeport APPORT ${APPORT}
###  -u UNDEPLOY KUBECTL DELETE ${UNDEPLOY}
###  -s SETUP & VERIFY ${SETUP}
###  -v VERIFY ${VERIFY}
###  -t TEST MODE ${TESTMODE}
###  -h HELP MESSAGE
###
# pods "hostname && ls -ld /var/www/html-default && ls -ld /var/lib/mysql-default"
# pods "hostname && ls -ld /var/www/html-tz && ls -ld /var/lib/mysql-tz"
# pods "hostname && sudo rm -rf /var/www/html-default && sudo rm -rf /var/lib/mysql-default"
# pods "hostname && sudo rm -rf /var/www/html-tz && sudo rm -rf /var/lib/mysql-tz"
###
############################ GLOBAL LIST #########
APP=wp_mysql
APPLOC=./yaml/${NAMESPACE}
manifest=kustomization.yaml
kubesystem="kube-system"
thelist=()
bin_dir=/usr/local/bin/

######### TEST MODE SUPERCEEDS EVERYRHING ########
function do_test {
  echo "### TEST MODE ###"
  #setup
  #teardown

  #if [ ${UNDEPLOY} -gt 0 ];
  #  then un-go && pass do_test || fail do_test
  #  else    go && pass do_test || fail do_test
  #fi
#  if [ ${APPORT} -gt 0 ]; then do_nodeport create; fi

  #init && verify && check_podnetwork && check_manifest && pass "$(basename $0)" || fail "$(basename $0)"
}
##################################################
###
### LIST OF REMOTE DIRECTORY STRUCTURE
###
function init {
  echo init
  root="/"
  thelist=()
  thelist+=("${root}var/www")
  thelist+=("${root}var/www/html-${NAMESPACE}")
  thelist+=("${root}var/lib/mysql-${NAMESPACE}")
  good2 "Directories" "${thelist[@]}"
}
##################################################
function go {
  echo -e "Going for ${APP} @ namespace: ${NAMESPACE}\\n"
  if [ ${APPORT} -gt 0 ]; then do_nodeport create
    else if check_podnetwork; then deploy; fi
  fi
}
##################################################
function un-go {
  echo -e "Undeploying ${APP} @ namespace: ${NAMESPACE}\\n"
  if check_podnetwork; then remove; fi
  #do_nodeport delete
}
##################################################
function deploy {
  if check_manifest; then kubectl create -k ${APPLOC}/${APP}/; fi
}
##################################################
function remove {
  if check_manifest; then kubectl delete -k ${APPLOC}/${APP}/; fi
}
##################################################
function check_podnetwork {
  if kubectl get pods -n ${kubesystem}|grep calico >/dev/null; then return 0; fi
  error "deploy pod network" "kubectl create -f ${K_YAML_CALICO}" && return 1;
}
##################################################
function check_manifest {
  if [ -f ${APPLOC}/${APP}/${manifest} ]; then return 0; fi
  error "${manifest}" "not found in ${APPLOC}/${APP}/"; return 1
}
##################################################
function getthelist {
  alert2 "Directories" "${thelist[@]}"
}
##################################################
function setup {
  echo -e "Setting up ${APP} @ namespace: ${NAMESPACE}"
  CMD="sudo mkdir"
  if [ ${#thelist[@]} = 0 ]; then return 1; fi
  for d in ${thelist[@]}; do setup="${setup}${CMD} ${d} 2> /dev/null;"; done
    # echo "setup: ${setup}"
  ${bin_dir}/#{SSHPODNODES} "${setup}"
}
##################################################
function verify {
  echo Verifying for namespace: ${NAMESPACE}
  CMD="sudo mkdir"
  for d in ${thelist[@]}; do setup="${setup}${CMD} ${d} 2> /dev/null;"; done
  RC=0 && for d in ${thelist[@]}; do verify="${verify}if ! ls ${d} >/dev/null 2>&1; then echo -n FAILED@ && echo \\\$(hostname -f) && exit 1; fi;"; done
    # echo "verify: ${verify}"
  if ! ${bin_dir}/#{SSHPODNODES}  "${verify}"; then RC=99; fi;
  return ${RC}
}
##################################################
function teardown {
  alert2 "Removing" "${thelist[@]}"
  CMD="sudo rm -rf"
  if [ ${#thelist[@]} = 0 ]; then return 0; fi
  for d in ${thelist[@]}; do teardown="${teardown}${CMD} ${d};"; done
    # echo "teardown: ${teardown}"
  ${bin_dir}/#{SSHPODNODES}  "${teardown}";
}
##################################################
function do_nodeport {
if [ ${APPORT} -gt 0 ]; then
cat <<STP | kubectl ${1} -f -
kind: Service
apiVersion: v1
metadata:
  name: wordpress-service-np-${NAMESPACE}
  namespace: ${NAMESPACE}
  labels:
    name: wordpress
spec:
  type: NodePort
  ports:
  - port: ${APPORT}
    nodePort: '`expr 30937 + #{MIRROR_FORWARD_PORT_ADD}`'
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    app: wordpress
STP
fi

}
##### END OF APPORT #####

##########   ***** MAIN *****   ##################
################## INIT ##########################
init
################## TEST ##########################
if [ ${TESTMODE} -gt 0 ]; then
  do_test
else
################## VERIFY ########################
  if [ ${VERIFY} -gt 0 ]
    then verify && pass "$(basename $0)" || fail2 "$(basename $0):" "run setup with -s" && exit 2
  else
################## DELETE ########################
    if [ ${UNDEPLOY} -gt 0 ]
      then un-go
    else
################## SETUP #########################
      if [ ${SETUP} -gt 0 ]
        then setup && verify && pass "$(basename $0)"
      else
################### GO ###########################
        verify && pass "$(basename $0)" && go || fail2 "$(basename $0):" "run setup first with -s?" && exit 3
##################################################
fi;fi;fi;fi;

##################################################
##################################################
##################################################
# END
##################################################
' > /root/${WORDPRESS_MYSQL_APP_SCRIPT}






K8S_DASHBOARD_SCRIPT=dashboard_install
script_list+=("${K8S_DASHBOARD_SCRIPT}")
echo -e "############ write ${K8S_DASHBOARD_SCRIPT}"
echo '#!/bin/bash
source /etc/k.library
########################
###
###  -u UNDEPLOY KUBECTL DELETE
###  -k GET ADMIN TOKEN
###
########################
function deploy {
  if [ -n "${K_YAML_DASHBOARD}" ]; then
    kubectl create -f ${K_YAML_DASHBOARD}
    kubectl create -f ./yaml/kubernetes-dashboard/dashboard-service-np.yaml
    kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk \"{print \\$1}\")
  else
    echo env K_YAML_DASHBOARD not set
  fi
}
function un-go {
  kubectl delete -f ./yaml/kubernetes-dashboard/dashboard-service-np.yaml
  kubectl delete -f ${K_YAML_DASHBOARD}
}
function gettoken {
  kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk \"{print \\$1}\") | grep token \
    |grep token:|awk '\\\''{print $2}'\\\''

}

################## GO ############################
if [ ${UNDEPLOY} -gt 0 ];
  then un-go;
  else if [ ${TOKEN} -gt 0 ]; then gettoken;
    else deploy;
  fi
fi
##################################################
##################################################
##################################################
# END
##################################################
' > /root/${K8S_DASHBOARD_SCRIPT}





START_CLUSTER_SCRIPT=start_cluster_default_calico
script_list+=("${START_CLUSTER_SCRIPT}")
echo -e "############ write ${START_CLUSTER_SCRIPT} ############"
echo '#!/bin/bash
source /etc/k.library
########################
###
### INITS A CONTROL PLANE
### CREATES A POD NETWORK
### WRITES JOIN FILES TO USE
###
### PARM1: YML - IF NOT LOCAL eg A URL, THEN DOWNLOADS
###         IF PARM1 IS EMPTY THEN USES DEFAULT
###
########################
YAML=${1}
K_APISERVER="#{mstr_ip_pool[0]}"
K_PODNETWORK="#{POD_NETWORK}"

function init_control_plane {
  if [ -z ${YAML} ]; then default_calico; fi
  sudo su - -c "kubeadm init --apiserver-advertise-address=${K_APISERVER} --pod-network-cidr=${K_PODNETWORK}"
  rm -rf ~vagrant/.kube && mkdir /home/vagrant/.kube 2>/dev/null
  sudo su -c "cp /etc/kubernetes/admin.conf /home/vagrant/.kube/config"
  sudo su -c "chown -R vagrant:vagrant /home/vagrant/.kube"
}
function default_calico {
  YAML="/home/vagrant/yaml/calico_3.17.apiv1.yaml"
}

function create_pod_network {
  if [ ! -f ${YAML} ]; then error "no local YAML" "${YAML}" && exit 1; fi
  sudo su - vagrant -c "kubectl create -f ${YAML}"
}

function get_yaml {
  if [ ! -f ${YAML} ]; then echo Need local YAML getting.....;
    curl -s ${YAML} -O && YAML=./$(basename ${YAML})
    if [ -f ${YAML} ]; then echo Just got ${YAML}; fi
  fi
}

function create_token {
  sudo su - -c "kubeadm token create --print-join-command > /joincluster"
  if [ -f /joincluster ]; then echo wrote $(hostname):/joincluster; fi
}
function wrapup {
  #echo && echo -e "\\$K_APISERVER\\t${K_APISERVER}\\n\\$K_PODNETWORK\\t${K_PODNETWORK}\\n\\$YAML\\t${YAML}"
  alert2 "Install Kubernetes Dashboard" "./'${K8S_DASHBOARD_SCRIPT}'"
  alert2 "Join Nodes" "/usr/local/bin/#{SSHJOINNODES}"
}

################## GO ############################
if [ ${JOIN} -gt 0 ]
  then create_token && wrapup
else
  init_control_plane
  create_pod_network && create_token && wrapup
fi
##################################################

##################################################
##################################################
##################################################
# END
##################################################
' > /root/${START_CLUSTER_SCRIPT}





CREATE_CALICO_SCRIPT=podnetwork_calico
script_list+=("${CREATE_CALICO_SCRIPT}")
echo -e "############ write ${CREATE_CALICO_SCRIPT} ############"
echo '#!/bin/bash
source /etc/k.library
kubectl create -f ${K_YAML_CALICO}
##################################################
##################################################
##################################################
# END
##################################################
' > /root/${CREATE_CALICO_SCRIPT}





CLUSTER_SAMPLE_SCRIPT=cluster_sample
script_list+=("${CLUSTER_SAMPLE_SCRIPT}")
echo -e "############ write ${CLUSTER_SAMPLE_SCRIPT} ############"
echo '#!/bin/bash
########################
###
### CREATES CONTROL PLANE WITH DEFAULT CALICO YML
###
########################
LOG="${HOME}/cluster_initialized.log"
if [[ -z ${K_APISERVER} ]]; then K_APISERVER="#{mstr_ip_pool[0]}"; echo "K_APISERVER not found, using ${K_APISERVER}"; fi
if [[ -z ${K_PODNETWORK} ]]; then K_PODNETWORK="#{POD_NETWORK}"; echo "K_PODNETWORK not found, using ${K_PODNETWORK}"; fi

YAML="https://docs.projectcalico.org/v3.9/manifests/calico.yaml"
LOG="${HOME}/cluster_initialized.log"
echo >${LOG} | tee -a ${LOG} && echo -n "Creating Cluster: " | tee -a ${LOG} && date | tee -a ${LOG}

echo "running kubeadm init:" | tee -a ${LOG}

sudo su - -c "kubeadm init --apiserver-advertise-address=${K_APISERVER} \
   --pod-network-cidr=${K_PODNETWORK}" 2>&1 | tee -a ${LOG}
systemctl status kubelet|grep Active 2>&1 | tee -a ${LOG}

mkdir /home/vagrant/.kube 2>/dev/null
sudo su -c "cp /etc/kubernetes/admin.conf /home/vagrant/.kube/config" 2>&1 | tee -a ${LOG}
sudo su -c "chown -R vagrant:vagrant /home/vagrant/.kube" 2>&1 | tee -a ${LOG}

echo "kubectl create:" | tee -a ${LOG}
sudo su - vagrant -c "kubectl create -f ${YAML}" 2>&1 | tee -a ${LOG}

echo "kubeadm token create:" >> ${LOG}
sudo su - -c "kubeadm token create --print-join-command > /joincluster" 2>&1 | tee -a ${LOG}

if [ -f /joincluster ]; then echo wrote /joincluster; fi
##################################################
##################################################
##################################################
# END
##################################################
' > /root/${CLUSTER_SAMPLE_SCRIPT}






################################################
###
### SET PERMISSIONS FOR SCRIPTS IN script_list
###
################################################
for script in ${script_list[@]}; do
   chown vagrant:vagrant /root/${script}
   chmod 744 /root/${script}
   ln -fn /root/${script} /home/vagrant/${script}
done
################################################

EOF
####################################################
# END $CREATE_CLUSTER
####################################################

###
### MASTER ###
###
   config.vm.define "#{MASTER}" do |subconfig|
      PORT = 2300 + MIRROR_SSH_PORT_ADD
      $NTP = $ETC_INITD_NTPSYNC + <<-EOF
#{NTP}
      EOF
      subconfig.vm.provision "SyncAll", type: "shell", run: "never", name: "SyncAll", inline: $ETC_HOSTS + $WRITE_SYNC + <<-EOF
/bin/bash /etc/init/synchosts.sh
      EOF
      subconfig.vm.provision "SyncTest", type: "shell", run: "never", name: "SyncTest", inline: $SYNCTEST_MASTER_ONLY
      subconfig.vm.provision "First", type: "shell", run: "once", name: "First", inline: $WRITE_HOSTS_APTUPDATE_PYTHON
      subconfig.vm.provision "GenKey", type: "shell", run: "once", name: "GenKey", inline: $GENKEY
      subconfig.vm.provision "RootKey", type: "shell", run: "once", name: "RootKey", path: "#{ROOT_KEY_SCRIPT}"
      subconfig.vm.provision "VagrantKey", type: "shell", run: "once", name: "VagrantKey", path: "#{VAGRANT_KEY_SCRIPT}"
      subconfig.vm.provision "init", type: "shell", run: "once", name: "init", path: "#{MASTER_SCRIPT}"
      subconfig.vm.provision "Software", type: "shell", run: "once", name: "Software", inline: <<-EOF
apt-get -qq install unzip nmap jq
#{NTP}
#{CONTAINERD}
#{CONTAINER_CONFIG}
#{K8S}
      EOF
###
### MASTER ###
###
      subconfig.vm.provision "bashrc", type: "shell", run: "once", name: "bashrc", inline: $APPEND_ETC_BASHRC
      subconfig.vm.provision "Cluster", type: "shell", run: "once", name: "Cluster", inline: $CREATE_CLUSTER
      subconfig.vm.provision "Last", type: "shell", run: "once", name: "Last", inline: $WRITE_SYNC
      subconfig.vm.provision "Final", type: "shell", run: "once", name:"Final", inline: <<-EOF
echo 3 | sudo update-alternatives --config editor >/dev/null
apt-get -qq clean
for u in `ls -C /home`;do
   bashrc=/home/${u}/.bashrc;
   echo '### START BASHRC' >> ${bashrc}
   x=`grep -n "### START BASHRC" ${bashrc} | head -1 | cut -f1 -d:` \
      && y=`cat ${bashrc}|wc -l` \
      && z=`expr $y - $x` \
      && cp -p ${bashrc} ${bashrc}.bak \
      && cat>${bashrc}<<<`head -n -${z} ${bashrc}.bak`
   echo 'alias ls="ls -Ahltr --color=auto"' >> ${bashrc}
   echo 'export PS1="\\[\\e]0;\\u@\\h: \\w\\a\\]${debian_chroot:+($debian_chroot)}\\[\\033[1;34m\\]\\u\\[\\033[0m\\]@\\[\\033[1;31m\\]\\h:\\[\\033[0;37m\\]\\w\\[\\033[0m\\]\\$ "' >> ${bashrc}
   ln -fsn /vagrant/_shared_assets/ /home/${u}/yaml
   echo set nu > /home/${u}/.vimrc
done
echo 'alias ls="ls -Ahltr --color=auto"' >> /root/.bashrc
echo 'export PS1="\\[\\e]0;\\u@\\h: \\w\\a\\]${debian_chroot:+($debian_chroot)}\\[\\033[1;32m\\]\\u\\[\\033[0m\\]@\\[\\033[1;31m\\]\\h:\\[\\033[0;37m\\]\\w\\[\\033[0m\\]\\$ "' >> /root/.bashrc
echo 'PATH=$PATH:/root/Docker' >> /root/.bashrc

date >> /etc/vagrant_provisioned_at
         EOF

          ### 30937 is app wp_mysql
        subconfig.vm.network "forwarded_port", guest: 30937, host: 30937 + MIRROR_FORWARD_PORT_ADD
          ### 30002 is app kubernetes-dashboard
        subconfig.vm.network "forwarded_port", guest: 30002, host: 30002 + MIRROR_FORWARD_PORT_ADD
        subconfig.vm.network "forwarded_port", guest: 8001, host: 8001 + MIRROR_FORWARD_PORT_ADD
        subconfig.vm.network "forwarded_port", guest: 32370, host: 32370 + MIRROR_FORWARD_PORT_ADD
        subconfig.vm.network "forwarded_port", guest: 31162, host: 31162 + MIRROR_FORWARD_PORT_ADD
###
### MASTER ### PORTS
###
      subconfig.vm.network "forwarded_port", guest: 22, host: PORT, id: "ssh", auto_correct: true
      subconfig.vm.network "private_network", ip: mstr_ip_pool[0], netmask: NODE_MASK, virtualbox__intnet: "#{File.basename(Dir.getwd)}#{TEST}"
      subconfig.vm.synced_folder "_shared_assets/", "/home/vagrant/_assets"
      subconfig.vm.synced_folder "/Users/brandonindia/Docker", "/root/Docker"
      subconfig.vm.synced_folder "/Users/brandonindia/Docker/assets.docker", "/root/assets.docker"

      subconfig.vm.box = "#{IMAGE}"
      subconfig.vm.hostname = "#{PURPOSE}-#{MASTER}.#{LIFECYCLE}.#{DOMAIN}"
      subconfig.vm.provider "#{PROVIDER}" do |v|
#         v.customize ["modifyvm", :id, "--natdnshostresolver1", "on"]
#         v.customize ["modifyvm", :id, "--natdnsproxy1", "on"]
         v.name = "#{VM_NAME_MASTER}"
         v.memory = 2048
         v.cpus = 2
      end
      subconfig.vm.provision "adhoc", type: "shell", run: "never", name: "adhoc", inline: $APPEND_ETC_BASHRC + <<-EOF
            #{NOTHING}
            echo 'PATH=$PATH:/root/Docker' >> /root/.bashrc
      EOF
   end
### END MASTER ###

###
### PODS ###
###

# $JOIN
####################################################
# =>        writes file: /root/join_k8s_cluster.sh
# =>        Changes owner to vagrant
# =>        Hard links to ~vagrant
####################################################
         $JOIN = <<-EOF
echo '#!/bin/bash
sudo su - -c "scp -o StrictHostKeyChecking=no \'#{MASTER}.#{HOSTNAME_SUFFIX}\':/joincluster /joincluster"
sudo su - -c "/bin/bash /joincluster"' > /root/joincluster

chown vagrant:vagrant /root/joincluster
ln -fn /root/joincluster /home/vagrant/joincluster
chmod 744 /home/vagrant/joincluster

EOF
###
### PODS ###
###
   (1..POD_CT).each do |i|
      config.vm.define "#{NODE}#{i}" do |subconfig|
MIRROR_SSH_PORT_ADD = 0; if PURPOSE.include? "mirror"; MIRROR_SSH_PORT_ADD += 10000; end
      PORT = 2300 + i + MIRROR_SSH_PORT_ADD
         subconfig.vm.provision "SyncAll", type: "shell", run: "never", name: "SyncAll", inline: $ETC_HOSTS + $WRITE_SYNC + <<-EOF
/bin/bash /etc/init/synchosts.sh
      EOF
         subconfig.vm.provision "First", type: "shell", run: "once", name: "First", inline: $WRITE_HOSTS_APTUPDATE_PYTHON
         subconfig.vm.provision "GenKey", type: "shell", run: "once", name: "GenKey", inline: $GENKEY
         subconfig.vm.provision "RootKey", type: "shell", run: "once", name: "RootKey", path: "#{ROOT_KEY_SCRIPT}"
         subconfig.vm.provision "VagrantKey", type: "shell", run: "once", name: "VagrantKey", path: "#{VAGRANT_KEY_SCRIPT}"
         subconfig.vm.provision "init", type: "shell", run: "once", name: "init", path: "#{NODE_SCRIPT}"
         subconfig.vm.provision "Software", type: "shell", run: "once", name: "Software", inline: <<-EOF
#{CONTAINERD}
#{CONTAINER_CONFIG}
#{K8S}
         EOF
         subconfig.vm.provision "Join", type: "shell", run: "once", name: "Join", inline: $JOIN
         subconfig.vm.provision "Last", type: "shell", run: "once", name: "Last", inline: $WRITE_SYNC
         subconfig.vm.provision "Final", type: "shell", run: "once", name:"Final", inline: <<-EOF
echo 3 | sudo update-alternatives --config editor >/dev/null
for u in `ls -C /home`;do
 echo 'alias ls="ls -Ahltr --color=auto"' >> /home/$u/.bashrc;
 echo alias kreset=\\"sudo su - -c \\\'kubeadm reset -f\\\' \\&\\& sudo rm -rf /joincluster \\&\\& sudo rm -rf ${HOME}/.kube \\&\\& sudo rm -rf /etc/cni/net.d\\" >> /home/$u/.bashrc;
 date >> /etc/vagrant_provisioned_at
done
      EOF
      subconfig.vm.provision "adhoc", type: "shell", run: "never", name: "adhoc", inline: <<-EOF
          #{NOTHING}
kubeadm reset -f
K8S_VERSION=#{K8S_VERSION_20}
systemctl stop kubelet
apt-get remove -qq kubelet=${K8S_VERSION}          kubeadm=${K8S_VERSION}          kubectl=${K8S_VERSION}          kubernetes-cni
rm -rf /opt/cni/bin/*
apt-get -qq update
K8S_VERSION=#{K8S_VERSION_19}
apt-get install -qq          kubelet=${K8S_VERSION}          kubeadm=${K8S_VERSION}          kubectl=${K8S_VERSION}          kubernetes-cni
systemctl enable kubelet
systemctl start kubelet

      EOF
###
### PODS
###
         ip_ndx = i - 1
         subconfig.vm.network "forwarded_port", guest: 22, host: PORT, id: "ssh", auto_correct: true
         subconfig.vm.network "private_network", ip: pod_ip_pool[ip_ndx], netmask: NODE_MASK, virtualbox__intnet: "#{File.basename(Dir.getwd)}#{TEST}"

         subconfig.vm.box = "#{IMAGE}"
         subconfig.vm.hostname = "#{PURPOSE}-#{NODE}#{i}.#{LIFECYCLE}.#{DOMAIN}"
         subconfig.vm.provider "#{PROVIDER}" do |v|
            v.name = "#{VM_NAME_NODE}#{i}"
            v.memory = 1024
            v.cpus = 2
         end
      end
   end
### END PODS ###

###########
#### SANDBOXES #####
   SAND_CT=0
   (1..SAND_CT).each do |i|
   config.vm.define "sandbox#{i}" do |subconfig|
      subconfig.vm.box = "#{IMAGE}"
      subconfig.vm.hostname = "#{PURPOSE}-SANDBOX#{i}.#{LIFECYCLE}.#{DOMAIN}"
      subconfig.vm.provider "#{PROVIDER}" do |v|
         v.name = "#{VM_NAME_base}SANDBOX#{i}"
         v.memory = 1024
         v.cpus = 2
      end
      subconfig.vm.provision "RootKey", type: "shell", run: "once", name: "RootKey", path: "#{ROOT_KEY_SCRIPT}"
      subconfig.vm.provision "VagrantKey", type: "shell", run: "once", name: "VagrantKey", path: "#{VAGRANT_KEY_SCRIPT}"
      subconfig.vm.provision "adhoc", type: "shell", run: "never", name: "adhoc", inline: <<-EOF
          #{NOTHING}
      EOF
   end
   end
#### END SANDBOXES #####

end
###
### END Vagrant.configure ###
###

MSG = "***** %%%%% ***** %%%%% ********** %%%%% ***** %%%%% *****"
   def msg(message)
      return "#{MSG} #{message}"
   end

###
### SOFTWARE ###
###

K8S_VERSION_20="1.20.2-00"
K8S_VERSION_19="1.19.7-00"
K8S_VERSION_18="1.18.15-00"
K8S_VERSION_17="1.17.17-00"

DOCKERVERSIONS="apt-cache madison docker-ce docker-ce-cli"
DOCKER_VERSION_5_20="5:20.10.2~3-0~ubuntu-bionic"
DOCKER_VERSION_5_19="5:19.03.14~3-0~ubuntu-bionic"

K8S_VERSION=K8S_VERSION_20
DOCKER_VERSION=DOCKER_VERSION_5_20

NTP = 'echo ' + msg("Installing NTP")+'
apt-get -y -qq install ntp ntpdate ntpstat \
&& if service ntp status > /dev/null; then service ntp stop; fi \
&& echo "syncing NTP with time.nist.gov..." \
&& ntpdate time.nist.gov && service ntp start \
&& ntp_tries=6 && ntp_delay_seconds=8 && i=0 \
&& while ! ntpstat; do sleep ${ntp_delay_seconds} && i=`expr ${i} + 1` \
&&    if [ ${i} -ge ${ntp_tries} ]; \
then echo -n "NTP:" && echo bailing && break; fi; done'

PYTHON = 'echo ' + msg("Installing Python")+'
apt-get install -qq python3 \
&& update-alternatives --install \
/usr/bin/python python /usr/bin/python3 1'

ANSIBLE = 'echo ' + msg("Installing Ansible")+'
apt-add-repository --yes --update ppa:ansible/ansible\
&& apt-get install -qq ansible'

AWSCLI = 'echo ' + msg("Installing AWS CLI")+'
if ! command -v curl > /dev/null; then apt-get install -qq curl; fi
if ! command -v unzip > /dev/null; then apt-get install -qq unzip; fi
curl -s https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip\
&& if ! command -v aws > /dev/null; then unzip -q awscliv2.zip && ./aws/install; else ./aws/install --update; fi'

DOCKER = 'DOCKER_VERSION="' + "noversion" + '" && echo ' + msg("Installing Docker version: ${DOCKER_VERSION}")+'
apt-get -qq update
if ! command -v curl > /dev/null; then apt-get install -qq curl; fi
apt-get install -qq \
apt-transport-https \
ca-certificates \
gnupg-agent \
software-properties-common \
&& curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - \
&& apt-key fingerprint 0EBFCD88 \
&& add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" \
&& apt-get -qq update \
&& apt-get -qq install \
docker-ce \
docker-ce-cli'

CONTAINERD = 'echo ' + msg("Installing CONTAINERD")+'
apt-get -qq update
if ! command -v curl > /dev/null; then apt-get install -qq curl; fi
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - \
&& apt-key fingerprint 0EBFCD88 \
&& add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" \
&& apt-get -qq update \
&& apt-get install -qq \
apt-transport-https \
ca-certificates \
gnupg2 \
software-properties-common \
containerd.io'

CONTAINER_CONFIG ='
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

sudo sed -i "/swap/d" /etc/fstab && sudo swapoff -a
sudo sysctl --system
sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml
sudo systemctl restart containerd
'

CONTAINER_CONFIG1 ='
cat >/etc/sysctl.d/kubernetes.conf<<EOF &&
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
sysctl --system >/dev/null 2>&1 \
&& sed -i "/swap/d" /etc/fstab && swapoff -a'

DOCKERCOMPOSE = 'echo ' + msg("Installing Docker Compose")+'
curl -sL https://github.com/docker/compose/releases/download/1.21.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose \
&& chmod +x /usr/local/bin/docker-compose'



K8S = 'K8S_VERSION="' + "#{K8S_VERSION}" + '" && echo ' + msg("Installing k8s version: ${K8S_VERSION}")+'
echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list \
&& curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \
&& apt-get -qq update \
&& apt-get install -qq \
kubelet=${K8S_VERSION} \
kubeadm=${K8S_VERSION} \
kubectl=${K8S_VERSION} \
kubernetes-cni
systemctl enable kubelet >/dev/null 2>&1
systemctl start kubelet >/dev/null 2>&1'

HELM = '
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash \
   && cd -
'

# bash.library
BASHLIBRARY = "echo '
function color   { echo -n \"\$(tput setaf $1;tput setab $2)${3}$(tput sgr 0) \"; }
function green   { color 4 2 \"${*}\"; }
function yellow  { color 0 3 \"${*}\"; }
function red     { color 0 1 \"${*}\"; }
function blue    { color 6 4 \"${*}\"; }
function cyan    { color 4 6 \"${*}\"; }
function grey    { color 0 7 \"${*}\"; }
function pass    { echo \"$(green PASS: ${*})\"; echo; }
function pass2   { echo -n \"$(green PASS: ${1})\"; echo ${*:2}; }
function warn    { echo \"$(yellow PASS: ${*})\"; echo; }
function warn2   { echo -n \"$(yellow PASS: ${1})\"; echo ${*:2}; }
function fail    { echo \"$(red FAIL: ${*})\"; echo; }
function fail2   { echo -n \"$(red FAIL: ${1})\"; echo ${*:2}; }
function info    { echo \"$(grey INFO: ${*})\"; echo; }
function info2   { echo -n \"$(red INFO: ${1})\"; echo ${*:2}; }
function println { printf \"$1\n\" \"${@:2}\"; }
function error   { println '" + '\\\'' + "'\t\e[31mError (%s): \e[m%s'" + '\\\'' + "' \"$1\" \"${*:2}\"; }
function error2  { println '" + '\\\'' + "'\t\e[31mError (%d): \e[m%s'" + '\\\'' + "' \"$1\" \"${*:2}\"; }
function good    { println '" + '\\\'' + "'\t\e[32m%s\e[m'" + '\\\'' + "' \"$*\"; }
function good2   { println '" + '\\\'' + "'\t\e[32m(%s) \e[m%s'" + '\\\'' + "' \"$1\" \"${*:2}\"; }
function alert   { println '" + '\\\'' + "'\t\e[33m%s\e[m'" + '\\\'' + "' \"$*\"; }
function alert2  { println '" + '\\\'' + "'\t\e[33m(%s) \e[m%s'" + '\\\'' + "' \"$1\" \"${*:2}\"; }
function showalias { echo ${BASH_ALIASES[*]}; }
function getservices { systemctl list-units --type=service; }
function getactive { systemctl list-units --type=service --state=active; }
function getinactive { systemctl list-units --type=service --state=inactive; }
function getdead { getinactive|grep dead; }
function getrunning { systemctl list-units --type=service --state=running; }
function verify {
   if [ -f ${1} ]; then echo ${1} exists
   else echo fail; return 1; fi
}
'"

# k.library
KLIBRARY = "echo '
function usage {
   _usage_ ${*}
}

_optlist_=()
_optlist_+=(\"n<namespace>\")
_optlist_+=(\"o<nodeport>\")
_optlist_+=(\"u\")
_optlist_+=(\"t\")
_optlist_+=(\"v\")
_optlist_+=(\"s\")
_optlist_+=(\"j\")
_optlist_+=(\"k\")
options_msg=\"options: \"
for o in ${_optlist_[@]}; do
  options_msg=\"${options_msg}${o}|\"
done
usage_msg=\"$(basename $0) [${options_msg}h]\"
function _usage_ {
  error usage ${usage_msg}
  echo -e \"${*}\"
}
function _help_ {
  alert ${usage_msg}
  println '" + '\\\'' + "'\t\e[33m-n namespace: NAMESPACE\e[m'" + '\\\'' + "' \"$*\";
  println '" + '\\\'' + "'\t\e[33m-o port: APPORT\e[m'" + '\\\'' + "' \"$*\";
  println '" + '\\\'' + "'\t\e[33m-u: UNDEPLOY\e[m'" + '\\\'' + "' \"$*\";
  println '" + '\\\'' + "'\t\e[33m-t: TEST\e[m'" + '\\\'' + "' \"$*\";
  println '" + '\\\'' + "'\t\e[33m-v: VERIFY\e[m'" + '\\\'' + "' \"$*\";
  println '" + '\\\'' + "'\t\e[33m-s: SETUP\e[m'" + '\\\'' + "' \"$*\";
  println '" + '\\\'' + "'\t\e[33m-j: JOIN\e[m'" + '\\\'' + "' \"$*\";
  println '" + '\\\'' + "'\t\e[33m-k: TOKEN\e[m'" + '\\\'' + "' \"$*\";
}

TESTMODE=0
SETUP=0
UNDEPLOY=0
NAMESPACE=default
APPORT=0
VERIFY=0
JOIN=0
TOKEN=0

_opts_=\"n:o:stuvjhk\"
while getopts ${_opts_} _option_
do
case \"${_option_}\"
in
n) NAMESPACE=${OPTARG};;
o) APPORT=${OPTARG};;
u) UNDEPLOY=1;;
t) TESTMODE=1;;
v) VERIFY=1;;
s) SETUP=1;;
j) JOIN=1;;
k) TOKEN=1;;
H1) _help_ \"NO ARGS\" && exit 0;;
H0) usage \"NO ARGS\" \": go\\n-n namespace: NAMESPACE\\n-o port: APPORT\\n-u: UNDEPLOY\\n-t: TEST\\n-s: SETUP\\n-v: VERIFY\\n-j: JOIN\\n-k: TOKEN\" && exit 0;;
h) _help_ && exit 0;;
*) usage && exit 1;;
esac
done
'"

LIBRARY=BASHLIBRARY+KLIBRARY
BASHRC = "echo '### START BASHRC
source /etc/k.library


alias watch=\"watch \"

alias khelp=\"kubectl api-resources\"
alias k=\"kubectl\"
alias kinfoall=\"kubectl get all --all-namespaces --output=wide\"
alias kinfoallw=\"watch kubectl get all --all-namespaces --output=wide\"
alias kinfo=\"kubectl get all --output=wide\"
alias kinfow=\"watch kubectl get all --output=wide\"

alias kcluster=\"kubectl cluster-info\"

alias   kgetpod=\"kubectl get pod\"
alias  kgetpods=\"kubectl get pods --all-namespaces -o wide\"
alias   kgetpodw=\"watch kubectl get pod\"
alias  kgetpodsw=\"watch kubectl get pods --all-namespaces\"

alias  knodew=\"kget no -o wide --watch\"
alias   kpvw=\"kget pc -o wide --watch\"

alias   kpodw=\"kget pod -o wide --watch\"
alias  kpodwa=\"kget pod -o wide -A --watch\"
alias   ksvcw=\"kget svc -o wide --watch\"
alias  ksvcwa=\"kget svc -o wide -A --watch\"
alias   kdeployw=\"kget deploy -o wide --watch\"
alias  kdeploywa=\"kget deploy -o wide -A --watch\"

alias   kpvcw=\"kget pvc -o wide --watch\"
alias  kpvcwa=\"kget pvc -o wide -A --watch\"
alias   krsw=\"kget rs -o wide --watch\"
alias  krswa=\"kget rs -o wide -A --watch\"
alias   kstsw=\"kget sts -o wide --watch\"
alias  kstswa=\"kget sts -o wide -A --watch\"

alias  kgetnode=\"kubectl get node\"
alias kgetnodes=\"kubectl get node --all-namespaces\"
alias  kgetnodew=\"watch kubectl get node\"
alias kgetnodesw=\"watch kubectl get node --all-namespaces\"

alias kgetns=\"kubectl get namespaces --show-labels\"

alias kget=\"kubectl get\"
alias kgetall=\"kubectl get --all-namespaces --output=wide\"
alias kdesc=\"kubectl describe\"
alias kgetconfig=\"kubectl config view\"

alias kdrn=\"echo Usage: kubectl drain \\\<node-name\\\> --ignore-daemonsets --delete-emptydir-data\"
alias kdlt=\"echo Usage: kubectl delete node \\\<node-name\\\>\"
function kdrain0 { for n in \${@}; do kubectl drain \${n} --delete-local-data=true --ignore-daemonsets; done; }
function kdrain1 { for n in \${@}; do kubectl drain \${n} --delete-emptydir-data --ignore-daemonsets; done; }
function kdelete { for n in \${@}; do kubectl delete node \${n}; done  ; }
function kdrainall0  { kdrain0 \$(kubectl get nodes --no-headers|grep -v master|awk '" + '\\\'' + "'{print \$1}'" + '\\\'' + "') ; }
function kdrainall1  { kdrain1 \$(kubectl get nodes --no-headers|grep -v master|awk '" + '\\\'' + "'{print \$1}'" + '\\\'' + "') ; }
function kdrainall  { kubectl get nodes --no-headers|grep -v master|awk '" + '\\\'' + "'{print \$1}'" + '\\\'' + "'|xargs kubectl drain --delete-emptydir-data --ignore-daemonsets ; }
function kdeleteall { kubectl get nodes --no-headers|grep -v master|awk '" + '\\\'' + "'{print \$1}'" + '\\\'' + "'|xargs kubectl delete node ; }

function ksetns { if [ -n \${1} ];then kubectl config set-context --current --namespace=\${1};else kubectl config set-context --current --namespace=default; fi ; }
alias kreset=\"sudo su - -c '" + '\\\'' + "'kubeadm reset -f'" + '\\\'' + "' \&\& sudo rm -rf /joincluster \&\& sudo rm -rf ${HOME}/.kube \&\& sudo rm -rf /etc/cni/net.d\"


info $(grep \"DISTRIB_DESCRIPTION\" /etc/lsb-release)
blue \"IP Route:\" && echo && ip route
blue \"Interfaces:\" && echo && ifconfig|grep -A2 enp

if command -v java > /dev/null 2>&1; then blue \"java:\" && java -version; else yellow \"No Java\"; fi;
if command -v javac > /dev/null 2>&1; then blue \"javac:\" && javac -version; else yellow \"No JDK\"; fi;
if command -v mvn > /dev/null 2>&1; then blue \"maven:\" && mvn --version; else yellow \"No Maven\"; fi;
if command -v hadoop > /dev/null; then blue \"Hadoop:\" && hadoop version; fi && echo;
if systemctl --all --type service | grep -q ntp.service; then \
   if systemctl list-units --type=service --state=inactive|grep ntp.service| grep -q dead; then
      sudo systemctl start ntp.service; fi
   echo \"NTP sync...\"
   if command -v ntpstat > /dev/null 2>&1; then
      ntp_tries=6 && ntp_delay_seconds=8 && i=0
      while ! ntpstat 2> /dev/null
        do sleep \${ntp_delay_seconds} && i=\`expr \${i} + 1\`
        if [ \${i} -ge \${ntp_tries} ]
          then yellow \"NTP:\" && echo bailing && break
        fi
      done
      if ntpstat > /dev/null 2>&1
         then green \"NTP Synchronization Status:\" && ntpstat
         else red \"NTP:\" && echo \"not synchronized\"
      fi
   fi
fi

if command -v kubectl > /dev/null 2>&1; then cyan \"kubectl:\" && kubectl version --short --client; else yellow \"No kubectl\"; fi;
if command -v kubelet > /dev/null 2>&1; then cyan \"kubelet:\" && kubelet --version; else yellow \"No kubelet\"; fi;
if command -v kubeadm > /dev/null 2>&1; then cyan \"kubeadm:\" && kubeadm version --output short; else yellow \"No kubeadm\"; fi;
if command -v docker > /dev/null 2>&1; then cyan \"docker:\" && docker --version && sudo docker run --rm hello-world 2> /dev/null | grep -o \"Hello from Docker!\"; else yellow \"No Docker\"; fi;
#if command -v helm > /dev/null 2>&1; then cyan \"helm:\" && helm version --short; else yellow \"No helm\"; fi && echo;
if command -v docker-compose > /dev/null 2>&1; then cyan \"Docker Compose:\" && docker-compose --version; fi;
if command -v aws > /dev/null 2>&1; then cyan \"AWS CLI:\" && /usr/local/bin/aws --version; else yellow \"No AWS CLI\"; fi && echo;

declare -A _sessvars_
export K_APISERVER=#{mstr_ip_pool[0]} && _sessvars_[K_APISERVER]=\${K_APISERVER}
export K_PODNETWORK=#{POD_NETWORK} && _sessvars_[K_PODNETWORK]=\${K_PODNETWORK}
export K_YAML_FLANNEL=https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml \
  && _sessvars_[K_YAML_FLANNEL]=\${K_YAML_FLANNEL}
export K_YAML_CALICO=https://docs.projectcalico.org/manifests/calico.yaml && _sessvars_[K_YAML_CALICO]=\${K_YAML_CALICO}
export K_YAML_CALICO_LOCAL=${HOME}/yaml/calico_3.17.apiv1.yaml && _sessvars_[K_YAML_CALICO_LOCAL]=\${K_YAML_CALICO_LOCAL}
export K_YAML_DASHBOARD=https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml \
   && _sessvars_[K_YAML_DASHBOARD]=\${K_YAML_DASHBOARD}
export KUBECONFIG=/home/vagrant/.kube/config && _sessvars_[KUBECONFIG]=\${KUBECONFIG}
#export K_YAML_=


echo && grey \"Session Variables\" && echo
for key in \${!_sessvars_[@]}; do cyan \"$key:\" && echo \${_sessvars_[\$key]}; done


echo
echo -n \"docker: \" && systemctl status docker | grep --color=never Active
echo -n \"containerd: \" && systemctl status containerd | grep --color=never Active
echo -n \"kubelet: \" && systemctl status kubelet | grep --color=never Active

'"

NOTHING = ""
