# -*- mode: ruby -*-
# vi: set ft=ruby :
### SET THIS TO -test (DASH TEST) TO ENABLE TEST MODE ###
TEST=""
INTERACTIVE = true
PURPOSE = "#{File.basename(Dir.getwd)}#{TEST}"
LIFECYCLE = "dev#{TEST}"
DOMAIN = "anytown.usa"
HOSTNAME_SUFFIX = "#{PURPOSE}.#{LIFECYCLE}.#{DOMAIN}"

POD_CT = File.read("./POD_CT").to_i
MSTR_CT = File.read("./MSTR_CT").to_i
CTLR_CT = File.read("./CTLR_CT").to_i
CONTROLLER = "controller"
MASTER = "master"
NODE = "node"

ROUTE="192.168.0.0"
ROUTE_NETMASK = 16
POD_NETWORK = "#{ROUTE}/#{ROUTE_NETMASK}"

NODE_PREFIX="192.168.0"
NODE_NETMASK = 16
mstr_ip_pool = ["#{NODE_PREFIX}.20"]
mstr_nm_pool = ["#{MASTER}"]
ctlr_ip_pool = []#["#{NODE_PREFIX}.10"]
ctlr_nm_pool = []#["#{CONTROLLER}"]
pod_ip_pool = []
pod_nm_pool = []

K8S_VERSION = "1.20.0-00" #1.17.16
DOCKER_VERSION = "5:19.03.14~3-0~ubuntu-bionic"

IMAGE = "ubuntu/bionic64"
PROVIDER = "virtualbox"
ROOT_KEY_SCRIPT = "root_key.sh"
VAGRANT_KEY_SCRIPT = "vagrant_key.sh"

NODE_SCRIPT = "node#{TEST}.sh"
MASTER_SCRIPT = "master#{TEST}.sh"
CONTROLLER_SCRIPT = "controller#{TEST}.sh"

VM_NAME_base = "#{PURPOSE}_#{LIFECYCLE}-"
VM_NAME_CONTROLLER = "#{PURPOSE}_#{LIFECYCLE}-#{CONTROLLER}"
VM_NAME_MASTER = "#{PURPOSE}_#{LIFECYCLE}-#{MASTER}"
VM_NAME_NODE = "#{PURPOSE}_#{LIFECYCLE}-#{NODE}"

VAGRANTFILE_API_VERSION = "2"
### START Vagrant.configure ###
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

IPLIST = ""
NAMELIST = ""
FQDN = ""
ETC_HOSTS = ""

for i in 1..POD_CT  do; pod_ip_pool.append("#{NODE_PREFIX}.#{i+100}"); pod_nm_pool.append("#{NODE}#{i}"); end

host_ip_pool = ctlr_ip_pool + mstr_ip_pool
host_nm_pool = ctlr_nm_pool + mstr_nm_pool

for i in (0..POD_CT - 1) do
   "#{host_ip_pool.append(pod_ip_pool[i])}"
   "#{host_nm_pool.append(pod_nm_pool[i])}"
end

CONTROLLERS=""
PODS=""
MASTERS=""
host_ip_pool.each do |ip| IPLIST+="#{ip} " end
for i in (0..CTLR_CT+MSTR_CT+POD_CT - 1) do
   NAMELIST += "#{host_nm_pool[i]} "
   FQDN += "#{host_nm_pool[i]}.#{PURPOSE}.#{LIFECYCLE}.#{DOMAIN} "
   ETC_HOSTS += "\n#{host_ip_pool[i]} #{host_nm_pool[i]}.#{PURPOSE}.#{LIFECYCLE}.#{DOMAIN} #{host_nm_pool[i]}"

   if "#{host_nm_pool[i]}".include? "#{NODE}"; PODS += "#{host_ip_pool[i]} " ; end
   if "#{host_nm_pool[i]}".include? "#{CONTROLLER}"; CONTROLLERS += "#{host_ip_pool[i]} " ; end
   if "#{host_nm_pool[i]}".include? "#{MASTER}"; MASTERS += "#{host_ip_pool[i]} " ; end
end

# WRITES:   /usr/local/bin/ssh-*
$BIN_RUNSSH = "echo #{MSG} Running: \\$BIN_RUNSSH

echo '#!/bin/bash
### ssh-all ####
if [ $# -gt 0 ]; then for ip in #{IPLIST}; do ssh ${ip} \"${1}\"; done;
else echo \" Usage: Need argument to send\"; fi
' > /usr/local/bin/ssh-all

echo '#!/bin/bash
### ssh-master ####
if [ $# -gt 0 ]; then for ip in #{MASTERS}; do ssh ${ip} \"${1}\"; done;
else echo \" Usage: Need argument to send\"; fi
' > /usr/local/bin/ssh-master

echo '#!/bin/bash
### ssh-pods ####
if [ $# -gt 0 ]; then for ip in #{PODS}; do ssh ${ip} \"${1}\"; done;
else echo \" Usage: Need argument to send\"; fi
' > /usr/local/bin/ssh-pods

chmod 700 /usr/local/bin/ssh-all /usr/local/bin/ssh-master /usr/local/bin/ssh-pods
chown vagrant:vagrant /usr/local/bin/ssh-all /usr/local/bin/ssh-master /usr/local/bin/ssh-pods
"

$BIN_RUNSSH1 = <<EOF

echo -e "#!/bin/bash
### ssh-pods-reset ####
if [ \\$\# -eq 0 ]; then for ip in #{PODS}; do echo Resetting pod @ \\${ip} && echo && ssh \\${ip} \\"sudo su - -c 'kubeadm reset -f && rm -rf /joincluster'\\"; done
else echo \\" Usage: Expect 0 args\\"; fi
sudo rm -rf /joincluster
" > /usr/local/bin/ssh-pods-reset

chmod 700 /usr/local/bin/ssh-pods-reset
chown vagrant:vagrant /usr/local/bin/ssh-pods-reset

echo -e "#!/bin/bash
### ssh-pods-join ####
if [ \\$\# -eq 0 ]; then for ip in #{PODS}; do ssh \\${ip} \\"echo && hostname -f && ./join\\"; done
else echo \\" Usage: Expect 0 args\\"; fi
" > /usr/local/bin/ssh-pods-join

chmod 700 /usr/local/bin/ssh-pods-join
chown vagrant:vagrant /usr/local/bin/ssh-pods-join
EOF

# APPENDS:  /etc/hosts
$ETC_HOSTS = <<EOF
x=`grep -n "127.0.2.1" /etc/hosts | cut -f1 -d:` \
   && y=`cat /etc/hosts|wc -l` \
   && z=`expr $y - $x` \
   && cp -p /etc/hosts /etc/hosts.bak \
   && cat>/etc/hosts<<<`head -n -${z} /etc/hosts.bak`
echo -e '#{ETC_HOSTS}' >> /etc/hosts
EOF

# WRITES:   /etc/init/synchosts.sh
$ETC_INIT_SYNCHOSTS = <<EOF
echo #{MSG} Running: \\$ETC_INIT_SYNCHOSTS
if [ ! -d /etc/init ]; then mkdir /etc/init && chmod 755 ${_}; fi
echo '#!/bin/bash
> /root/.ssh/known_hosts && chmod 600 /root/.ssh/known_hosts
for ip in #{IPLIST}; do
   ssh-keyscan -H ${ip} >> /root/.ssh/known_hosts; done
for nm in #{NAMELIST}; do
   ssh-keyscan -H ${nm} >> /root/.ssh/known_hosts; done
for nm in #{FQDN}; do
   ssh-keyscan -H ${nm} >> /root/.ssh/known_hosts; done
cp -f /root/.ssh/known_hosts /home/vagrant/.ssh/known_hosts \
   && chown vagrant:vagrant /home/vagrant/.ssh/known_hosts \
   && chmod 600 /home/vagrant/.ssh/known_hosts
' > /etc/init/synchosts.sh \
&& chmod 755 /etc/init/synchosts.sh
EOF

# WRITES:   /etc/init.d/ntpsync.sh
# LINKS:    /etc/rc3.d/S01ntpsync
$ETC_INITD_NTPSYNC = <<EOF
echo #{MSG} Running: \\$ETC_INITD_NTPSYNC
echo '#!/bin/bash

if ! ntpstat > /dev/null 2>&1
   then service ntp stop && ntpdate time.nist.gov && service ntp start
   ntp_tries=15 && ntp_delay_seconds=2 && i=0
   while ! ntpstat
     do sleep ${ntp_delay_seconds} && i=`expr ${i} + 1`
     if [ ${i} -ge ${ntp_tries} ]
       then yellow "NTP:" && echo bailing && break
     fi
   done
fi' > /etc/init.d/ntpsync.sh \
   && chmod 755 /etc/init.d/ntpsync.sh \
   && ln -sfn /etc/init.d/ntpsync.sh /etc/rc3.d/S01ntpsync
EOF

$SYNCTEST_MASTER_ONLY = <<EOF
echo #{MSG} Running: \\$SYNCTEST_MASTER_ONLY
echo -e "#!/bin/bash\nssh-pods 's=\\\`hostname -f\\\` && echo -n \\"\`hostname\` to \\\${s} -> Success\\" && s=\\" and \\\`hostname\\\` back for -> Success\\" && ssh master \\"echo -n \\\\\\"\\\${s}\\\\\\" && hostname -f\\"'
" > /root/test-node-connections \
   && chown vagrant:vagrant /root/test-node-connections \
   && chmod 744 /root/test-node-connections \
   && ln -fn /root/test-node-connections /home/vagrant/test-node-connections \
   && /root/test-node-connections

EOF

$APPEND_ETC_BASHRC = <<EOF
echo #{MSG} Running: \\$APPEND_ETC_BASHRC
f=/etc/bash.bashrc
echo '### START BASHRC' >> ${f}
x=`grep -n "### START BASHRC" ${f} | head -1 | cut -f1 -d:` \
   && y=`cat ${f}|wc -l` \
   && z=`expr $y - $x` \
   && cp -p ${f} ${f}.bak \
   && cat>${f}<<<`head -n -${z} ${f}.bak`
echo export HISTFILE=/vagrant/bash_history  >> ${f}
echo "echo -e \\"\\n### Login: \\`date\\` ###\\" >> \\${HISTFILE}"  >> ${f}
#{BASHRC}  >> ${f}
EOF

$GENKEY = <<EOF
ssh-keygen -t rsa -b 4096 -f "/root/.ssh/provisioned_id_rsa.`openssl rand -hex 12`" -q -N ""
ssh-keygen -t rsa -b 4096 -f "/home/vagrant/.ssh/provisioned_id_rsa.`openssl rand -hex 12`" -q -N ""
EOF

### WRITE /ETC/HOSTS VERY EARLY
$WRITE_HOSTS_APTUPDATE_PYTHON = <<EOF
sed -i 's/mesg\\ n\\ ||\\ true/test\\ -t\\ 0\\ \\&\\&\\ mesg\\ n/' /root/.profile
echo -e '#{ETC_HOSTS}' >> /etc/hosts \
   && echo -e 'added to /etc/hosts: #{ETC_HOSTS}'
apt-get -qq update
#{PYTHON}
EOF

$WRITE_SYNC = $ETC_INIT_SYNCHOSTS + $BIN_RUNSSH + $BIN_RUNSSH1 + <<-EOF
   date >> /etc/vagrant_provisioned_at
EOF

$ADHOC = ""

###
### MASTER ###
###

# $CREATE_CLUSTER
####################################################
# =>        writes file: /root/create_k8s_controlplane
# =>        Changes owner to vagrant
# =>        Hard links to ~vagrant
####################################################
      $CREATE_CLUSTER = <<-EOF
scripts_ary=()

echo -e "############ write deploy_wp_mysql ############"
echo '#!/bin/bash
########################
###
### CREATES CONTROL PLANE WITH CALICO
### JOINS NODE TO CLUSTER
### CREATES & DEPLOYS APP IN NAMESPACE
###
########################
./create_k8s_controlplane apply yaml/calico_3.17.apiv1.yaml
ssh-pods-join
kubectl apply -f ./yaml/techcraz/namespace.techcraz.yaml
kubectl apply -k ./yaml/techcraz/

' > /root/deploy_wp_mysql
scripts_ary+=("deploy_wp_mysql")



echo -e "############ write install_k8s_dashboard - not working"
echo '#!/bin/bash
########################
###
### ?????
###
########################
kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml

# Create an admin account called k8s-admin
kubectl --namespace kube-system create serviceaccount k8s-admin
kubectl create clusterrolebinding k8s-admin --serviceaccount=kube-system:k8s-admin --clusterrole=cluster-admin

# Setup a proxy to your workstation
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep k8s-admin | awk '{print \\$1}')

' > /root/install_k8s_dashboard
scripts_ary+=("install_k8s_dashboard")



echo -e "############ write create_k8s_controlplane ############"
echo '#!/bin/bash
########################
###
### CREATES CONTROL PLANE
### WRITES /joincluster FOR DOWNLOAD
###
### PARM1: apply | create
### PARM2: YML - IF NOT LOCAL IE A URL THEN DOWNLOADS AND EXITS
###
########################
CMD=${1}
YAML=${2}
K_APISERVER="#{mstr_ip_pool[0]}"
K_PODNETWORK="#{POD_NETWORK}"

echo && echo -e "\\$K_APISERVER\\t${K_APISERVER}\\n\\$K_PODNETWORK\\t${K_PODNETWORK}\\n\\$YAML\\t${YAML}"
if [ -z ${CMD} ];then echo -e "\\tUsage: $(basename \$0) [CMD] [rm YAML]\\n\\tdid you pass create or apply?" && exit 1; fi
if [ ! -f ${YAML} ]; then echo Need local YAML getting.....;
   curl -s ${YAML} -O && if [ -f ./$(basename ${YAML}) ]; then echo Just got ./$(basename ${YAML}); fi
exit 2; fi
echo using local ${YAML}

sudo su - -c "kubeadm init --apiserver-advertise-address=${K_APISERVER} --pod-network-cidr=${K_PODNETWORK}"
rm -rf ~vagrant/.kube && mkdir /home/vagrant/.kube 2>/dev/null
sudo su -c "cp /etc/kubernetes/admin.conf /home/vagrant/.kube/config"
sudo su -c "chown -R vagrant:vagrant /home/vagrant/.kube"

echo && systemctl status kubelet|grep Active

sudo su - vagrant -c "kubectl ${CMD} -f ${YAML}"
#if ! sudo su - vagrant -c "kubectl ${CMD} -f ${YAML}" 2> /dev/null; then ;fi

sudo su - -c "kubeadm token create --print-join-command > /joincluster"
if [ -f /joincluster ]; then echo && echo /joincluster exists for pods to use; fi

' > /root/create_k8s_controlplane
scripts_ary+=("create_k8s_controlplane")





echo -e "############ write create_k8s_controlplane_sample ############"
echo '#!/bin/bash
########################
###
### CREATES CONTROL PLANE WITH DEFAULT CALICO YML
###
########################
LOG="${HOME}/cluster_initialized.log"
if [[ -z ${K_APISERVER} ]]; then K_APISERVER="#{mstr_ip_pool[0]}"; echo "K_APISERVER not found, using ${K_APISERVER}"; fi
if [[ -z ${K_PODNETWORK} ]]; then K_PODNETWORK="#{POD_NETWORK}"; echo "K_PODNETWORK not found, using ${K_PODNETWORK}"; fi

YAML="https://docs.projectcalico.org/v3.9/manifests/calico.yaml"
LOG="${HOME}/cluster_initialized.log"
echo >${LOG} | tee -a ${LOG} && echo -n "Creating Cluster: " | tee -a ${LOG} && date | tee -a ${LOG}

echo "running kubeadm init:" | tee -a ${LOG}

sudo su - -c "kubeadm init --apiserver-advertise-address=${K_APISERVER} \
   --pod-network-cidr=${K_PODNETWORK}" 2>&1 | tee -a ${LOG}
systemctl status kubelet|grep Active 2>&1 | tee -a ${LOG}

mkdir /home/vagrant/.kube 2>/dev/null
sudo su -c "cp /etc/kubernetes/admin.conf /home/vagrant/.kube/config" 2>&1 | tee -a ${LOG}
sudo su -c "chown -R vagrant:vagrant /home/vagrant/.kube" 2>&1 | tee -a ${LOG}

echo "kubectl create:" | tee -a ${LOG}
sudo su - vagrant -c "kubectl create -f ${YAML}" 2>&1 | tee -a ${LOG}

echo "kubeadm token create:" >> ${LOG}
sudo su - -c "kubeadm token create --print-join-command > /joincluster" 2>&1 | tee -a ${LOG}

if [ -f /joincluster ]; then echo /joincluster exists for pods to use; fi

' > /root/create_k8s_controlplane_sample
scripts_ary+=("create_k8s_controlplane_sample")



echo '#!/bin/bash
while getopts c: option
do
case \"${option}\"
in
c) CP=${OPTARG};;
esac
done
' > /root/library
scripts_ary+=("library")

for s in ${scripts_ary[@]}; do
   chown vagrant:vagrant /root/${s}
   chmod 744 /root/${s}
   ln -fn /root/${s} /home/vagrant/${s}
done

EOF
####################################################
# END $CREATE_CLUSTER
####################################################

###
### MASTER ###
###
   config.vm.define "#{MASTER}" do |subconfig|
MIRROR_PORT_ADD = 0; if PURPOSE.include? "mirror"; MIRROR_PORT_ADD += 1000; end
      PORT = 2300 + MIRROR_PORT_ADD
      $NTP = $ETC_INITD_NTPSYNC + <<-EOF
#{NTP}
      EOF
      subconfig.vm.provision "Sync", type: "shell", run: "never", name: "Sync", inline: $ETC_HOSTS + $WRITE_SYNC + <<-EOF
/bin/bash /etc/init/synchosts.sh
      EOF
      subconfig.vm.provision "SyncTest", type: "shell", run: "never", name: "SyncTest", inline: $SYNCTEST_MASTER_ONLY
###
### MASTER ###
###
      subconfig.vm.provision "First", type: "shell", run: "once", name: "First", inline: $WRITE_HOSTS_APTUPDATE_PYTHON
      subconfig.vm.provision "GenKey", type: "shell", run: "once", name: "GenKey", inline: $GENKEY
      subconfig.vm.provision "RootKey", type: "shell", run: "once", name: "RootKey", path: "#{ROOT_KEY_SCRIPT}"
      subconfig.vm.provision "VagrantKey", type: "shell", run: "once", name: "VagrantKey", path: "#{VAGRANT_KEY_SCRIPT}"
      subconfig.vm.provision "init", type: "shell", run: "once", name: "init", path: "#{MASTER_SCRIPT}"
      subconfig.vm.provision "Software", type: "shell", run: "once", name: "Software", inline: <<-EOF
apt-get -qq install unzip
#{K8S}
#{DOCKER}
      EOF
      subconfig.vm.provision "bashrc", type: "shell", run: "once", name: "bashrc", inline: $APPEND_ETC_BASHRC
      subconfig.vm.provision "Cluster", type: "shell", run: "once", name: "Cluster", inline: $CREATE_CLUSTER
      subconfig.vm.provision "Last", type: "shell", run: "once", name: "Last", inline: $WRITE_SYNC
      subconfig.vm.provision "Final", type: "shell", run: "once", name:"Final", inline: <<-EOF
apt-get -qq clean
for u in `ls -C /home`;do
   f=/home/${u}/.bashrc;
   echo '### START BASHRC' >> ${f}
   x=`grep -n "### START BASHRC" ${f} | head -1 | cut -f1 -d:` \
      && y=`cat ${f}|wc -l` \
      && z=`expr $y - $x` \
      && cp -p ${f} ${f}.bak \
      && cat>${f}<<<`head -n -${z} ${f}.bak`
   echo 'alias ls="ls -Ahltr --color=auto"' >> /home/${u}/.bashrc;
   ln -fsn /vagrant/_shared_assets/ /home/${u}/yaml
done
echo 'alias ls="ls -Ahltr --color=auto"' >> /root/.bashrc

date >> /etc/vagrant_provisioned_at
         EOF
###
### MASTER ###
###
      if ! PURPOSE.include? "mirror"
         subconfig.vm.network "forwarded_port", guest: 8001, host: 8001
         subconfig.vm.network "forwarded_port", guest: 31254, host: 81
      end
      subconfig.vm.network "forwarded_port", guest: 22, host: PORT, id: "ssh", auto_correct: true
      subconfig.vm.network "private_network", ip: mstr_ip_pool[0], netmask: NODE_NETMASK, virtualbox__intnet: "#{File.basename(Dir.getwd)}#{TEST}"
      subconfig.vm.synced_folder "_shared_assets/", "/home/vagrant/_assets"

      subconfig.vm.box = "#{IMAGE}"
      subconfig.vm.hostname = "#{PURPOSE}-#{MASTER}.#{LIFECYCLE}.#{DOMAIN}"
      subconfig.vm.provider "#{PROVIDER}" do |v|
#         v.customize ["modifyvm", :id, "--natdnshostresolver1", "on"]
#         v.customize ["modifyvm", :id, "--natdnsproxy1", "on"]
         v.name = "#{VM_NAME_MASTER}"
         v.memory = 2048
         v.cpus = 2
      end
      subconfig.vm.provision "adhoc", type: "shell", run: "never", name: "adhoc", inline: <<-EOF
#{CONTAINERD}
EOF
   end
### END MASTER ###

###
### PODS ###
###

# $JOIN
####################################################
# =>        writes file: /root/join_k8s_cluster.sh
# =>        Changes owner to vagrant
# =>        Hard links to ~vagrant
####################################################
         $JOIN = <<-EOF
echo '#!/bin/bash
sudo su - -c "scp -o StrictHostKeyChecking=no \'#{MASTER}.#{HOSTNAME_SUFFIX}\':/joincluster /joincluster"
sudo su - -c "/bin/bash /joincluster"' > /root/join_k8s_cluster.sh

chown vagrant:vagrant /root/join_k8s_cluster.sh
ln -fn /root/join_k8s_cluster.sh /home/vagrant/join
chmod 744 /home/vagrant/join

EOF
###
### PODS ###
###
   (1..POD_CT).each do |i|
      config.vm.define "#{NODE}#{i}" do |subconfig|
MIRROR_PORT_ADD = 0; if PURPOSE.include? "mirror"; MIRROR_PORT_ADD += 1000; end
      PORT = 2300 + i + MIRROR_PORT_ADD
         subconfig.vm.provision "Sync", type: "shell", run: "never", name: "Sync", inline: $ETC_HOSTS + $ETC_INIT_SYNCHOSTS + <<-EOF
/bin/bash /etc/init/synchosts.sh
      EOF
         subconfig.vm.provision "First", type: "shell", run: "once", name: "First", inline: $WRITE_HOSTS_APTUPDATE_PYTHON
         subconfig.vm.provision "GenKey", type: "shell", run: "once", name: "GenKey", inline: $GENKEY
         subconfig.vm.provision "RootKey", type: "shell", run: "once", name: "RootKey", path: "#{ROOT_KEY_SCRIPT}"
         subconfig.vm.provision "VagrantKey", type: "shell", run: "once", name: "VagrantKey", path: "#{VAGRANT_KEY_SCRIPT}"
         subconfig.vm.provision "init", type: "shell", run: "once", name: "init", path: "#{NODE_SCRIPT}"
         subconfig.vm.provision "Software", type: "shell", run: "once", name: "Software", inline: <<-EOF
#{K8S}
#{DOCKER}
         EOF
         subconfig.vm.provision "Join", type: "shell", run: "once", name: "Join", inline: $JOIN
         subconfig.vm.provision "Last", type: "shell", run: "once", name: "Last", inline: $WRITE_SYNC
         subconfig.vm.provision "Final", type: "shell", run: "once", name:"Final", inline: <<-EOF
for u in `ls -C /home`;do
 echo 'alias ls="ls -Ahltr --color=auto"' >> /home/$u/.bashrc;
 echo alias kreset=\\"sudo su - -c \\\'kubeadm reset -f\\\' \\&\\& sudo rm -rf /joincluster \\&\\& sudo rm -rf ${HOME}/.kube \\&\\& sudo rm -rf /etc/cni/net.d\\" >> /home/$u/.bashrc;
 date >> /etc/vagrant_provisioned_at
done
      EOF
      subconfig.vm.provision "adhoc", type: "shell", run: "never", name: "adhoc", inline: <<-EOF
mv /home/vagrant/join_k8s_cluster.sh /home/vagrant/join
      EOF
###
### PODS
###
         ip_ndx = i - 1
         subconfig.vm.network "forwarded_port", guest: 22, host: PORT, id: "ssh", auto_correct: true
         subconfig.vm.network "private_network", ip: pod_ip_pool[ip_ndx], netmask: NODE_NETMASK, virtualbox__intnet: "#{File.basename(Dir.getwd)}#{TEST}"

         subconfig.vm.box = "#{IMAGE}"
         subconfig.vm.hostname = "#{PURPOSE}-#{NODE}#{i}.#{LIFECYCLE}.#{DOMAIN}"
         subconfig.vm.provider "#{PROVIDER}" do |v|
            v.name = "#{VM_NAME_NODE}#{i}"
            v.memory = 1024
            v.cpus = 2
         end
      end
   end
### END PODS ###

###########
#### SANDBOXES #####
   SAND_CT=2
   (1..SAND_CT).each do |i|
   config.vm.define "sandbox#{i}" do |subconfig|
      subconfig.vm.box = "#{IMAGE}"
      subconfig.vm.hostname = "#{PURPOSE}-SANDBOX#{i}.#{LIFECYCLE}.#{DOMAIN}"
      subconfig.vm.provider "#{PROVIDER}" do |v|
         v.name = "#{VM_NAME_base}SANDBOX#{i}"
         v.memory = 1024
         v.cpus = 2
      end
      subconfig.vm.provision "RootKey", type: "shell", run: "once", name: "RootKey", path: "#{ROOT_KEY_SCRIPT}"
      subconfig.vm.provision "VagrantKey", type: "shell", run: "once", name: "VagrantKey", path: "#{VAGRANT_KEY_SCRIPT}"
   end
   end
#### SEND SANDBOXES #####

end
###
### END Vagrant.configure ###
###

MSG = "%%%%% %%%%% %%%%% %%%%% %%%%% %%%%% %%%%% %%%%% %%%%% *****"
   def msg(message)
      return "#{MSG} #{message}"
   end

NTP = 'echo ' + msg("Installing NTP")+'
   apt-get -y -qq install ntp ntpdate ntpstat \
      && if service ntp status > /dev/null; then service ntp stop; fi \
      && echo "syncing NTP with time.nist.gov..." \
      && ntpdate time.nist.gov && service ntp start \
      && ntp_tries=10 && ntp_delay_seconds=3 && i=0 \
      && while ! ntpstat; do sleep ${ntp_delay_seconds} && i=`expr ${i} + 1` \
      &&    if [ ${i} -ge ${ntp_tries} ]; \
               then echo -n "NTP:" && echo bailing && break; fi; done'

PYTHON = 'echo ' + msg("Installing Python")+'
   apt-get install -qq python3 \
      && update-alternatives --install \
            /usr/bin/python python /usr/bin/python3 1'

ANSIBLE = 'echo ' + msg("Installing Ansible")+'
   apt-add-repository --yes --update ppa:ansible/ansible\
   && apt-get install -qq ansible'

AWSCLI = 'echo ' + msg("Installing AWS CLI")+'
   if ! command -v curl > /dev/null; then apt-get install -qq curl; fi
   if ! command -v unzip > /dev/null; then apt-get install -qq unzip; fi
   curl -s https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip\
   && if ! command -v aws > /dev/null; then unzip -q awscliv2.zip && ./aws/install; else ./aws/install --update; fi'

DOCKER = 'DOCKER_VERSION="' + "#{DOCKER_VERSION}" + '" && echo ' + msg("Installing Docker version: ${DOCKER_VERSION}")+'
   apt-get -qq update
   if ! command -v curl > /dev/null; then apt-get install -qq curl; fi
   apt-get install -qq \
         apt-transport-https \
         ca-certificates \
         gnupg-agent \
         software-properties-common \
      && curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - \
      && apt-key fingerprint 0EBFCD88 \
      && add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" \
      && apt-get -qq update \
      && apt-get install -qq \
         docker-ce=${DOCKER_VERSION} \
         docker-ce-cli=${DOCKER_VERSION} \
         containerd.io'


CONTAINERD = 'echo ' + msg("Installing CONTAINERD")+'
tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
modprobe overlay
modprobe br_netfilter
tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
sysctl --system
apt-get -qq update
   if ! command -v curl > /dev/null; then apt-get install -qq curl; fi
   curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - \
      && apt-key fingerprint 0EBFCD88 \
      && add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" \
      && apt-get -qq update \
      && apt-get install -qq \
         apt-transport-https \
         ca-certificates \
         gnupg-agent \
         software-properties-common \
         containerd.io
mkdir -p /etc/containerd
containerd config default  /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd'

DOCKERCOMPOSE = 'echo ' + msg("Installing Docker Compose")+'
   curl -sL https://github.com/docker/compose/releases/download/1.21.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose \
      && chmod +x /usr/local/bin/docker-compose'

K8S = 'K8S_VERSION="' + "#{K8S_VERSION}" + '" && echo ' + msg("Installing k8s version: ${K8S_VERSION}")+'
cat >/etc/sysctl.d/kubernetes.conf<<EOF &&
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
   sysctl --system >/dev/null 2>&1 \
      && sed -i "/swap/d" /etc/fstab && swapoff -a
   echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list \
      && curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \
      && apt-get -qq update \
      && apt-get install -qq \
         kubelet=${K8S_VERSION} \
         kubeadm=${K8S_VERSION} \
         kubectl=${K8S_VERSION} \
         kubernetes-cni
   systemctl enable kubelet >/dev/null 2>&1
   systemctl start kubelet >/dev/null 2>&1'

HELM = '
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash \
   && cd -
'

BASHRC = "echo '### START BASHRC
function color  { echo -n \"\$(tput setaf $1;tput setab $2)${3}$(tput sgr 0) \"; }
function green  { color 4 2 \"${*}\"; }
function yellow { color 0 3 \"${*}\"; }
function red    { color 0 1 \"${*}\"; }
function blue   { color 6 4 \"${*}\"; }
function cyan   { color 4 6 \"${*}\"; }
function grey   { color 0 7 \"${*}\"; }
function pass   { echo \"$(green PASS: ${*})\"; echo; }
function warn   { echo \"$(yellow PASS: ${*})\"; echo; }
function fail   { echo \"$(red FAIL: ${*})\"; echo; }
function info   { echo \"$(grey INFO: ${*})\"; echo; }
function showalias { echo ${BASH_ALIASES[*]}; }
function getservices { systemctl list-units --type=service; }
function getactive { systemctl list-units --type=service --state=active; }
function getinactive { systemctl list-units --type=service --state=inactive; }
function getdead { getinactive|grep dead; }
function getrunning { systemctl list-units --type=service --state=running; }
function verify {
   if [ -f ${1} ]; then echo ${1} exists
   else echo fail; return 1; fi
}

alias watch=\"watch \"

alias khelp=\"kubectl api-resources\"
alias k=\"kubectl\"
alias kinfoall=\"kubectl get all --all-namespaces --output=wide\"
alias kinfoallw=\"watch kubectl get all --all-namespaces --output=wide\"
alias kinfo=\"kubectl get all --output=wide\"
alias kinfow=\"watch kubectl get all --output=wide\"

alias kcluster=\"kubectl cluster-info\"

alias   kgetpod=\"kubectl get pod\"
alias  kgetpods=\"kubectl get pods --all-namespaces\"
alias   kgetpodw=\"watch kubectl get pod\"
alias  kgetpodsw=\"watch kubectl get pods --all-namespaces\"

alias  kgetnode=\"kubectl get node\"
alias kgetnodes=\"kubectl get node --all-namespaces\"
alias  kgetnodew=\"watch kubectl get node\"
alias kgetnodesw=\"watch kubectl get node --all-namespaces\"

alias kgetns=\"kubectl get namespaces --show-labels\"

alias kget=\"kubectl get\"
alias kgetall=\"kubectl get --all-namespaces --output=wide\"
alias kdesc=\"kubectl describe\"

alias kdrn=\"echo Usage: kubectl drain \\\<node-name\\\> --ignore-daemonsets --delete-emptydir-data\"
alias kdlt=\"echo Usage: kubectl delete node \\\<node-name\\\>\"
function kdrain  { for n in \${@}; do kubectl drain \${n} --delete-emptydir-data --ignore-daemonsets; done; }
function kdelete { for n in \${@}; do kubectl delete node \${n}; done  ; }

alias master=/usr/local/bin/ssh-master
alias pods=/usr/local/bin/ssh-pods
echo \"Aliased: master => ssh-master\"
echo \"Aliased: pods => ssh-pods\"

export K_APISERVER=#{mstr_ip_pool[0]}
export K_PODNETWORK=#{POD_NETWORK}
export YAML_FLANNEL=\"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\"
export YAML_CALICO=\"https://docs.projectcalico.org/manifests/calico.yaml\"
#export YAML_=\"\"



info $(grep \"DISTRIB_DESCRIPTION\" /etc/lsb-release)
blue \"IP Route:\" && echo && ip route
blue \"Interfaces:\" && echo && ifconfig|grep -A2 enp

if command -v java > /dev/null 2>&1; then blue \"java:\"; java -version; else yellow \"No Java\"; fi;
if command -v javac > /dev/null 2>&1; then blue \"javac:\"; javac -version; else yellow \"No JDK\"; fi;
if command -v mvn > /dev/null 2>&1; then blue \"maven:\"; mvn --version; else yellow \"No Maven\"; fi;
if command -v hadoop > /dev/null; then blue \"Hadoop:\";hadoop version; fi
if systemctl --all --type service | grep -q ntp.service; then \
   if systemctl list-units --type=service --state=inactive|grep ntp.service| grep -q dead; then
      sudo systemctl start ntp.service; fi
   echo \"NTP sync...\"
   if command -v ntpstat > /dev/null 2>&1; then
      ntp_tries=15 && ntp_delay_seconds=2 && i=0
      while ! ntpstat 2> /dev/null
        do sleep \${ntp_delay_seconds} && i=\`expr \${i} + 1\`
        if [ \${i} -ge \${ntp_tries} ]
          then yellow \"NTP:\" && echo bailing && break
        fi
      done
      if ntpstat > /dev/null 2>&1
         then green \"NTP Synchronization Status:\" && ntpstat
         else red \"NTP:\" && echo \"not synchronized\"
      fi
   fi
fi

if command -v kubectl > /dev/null 2>&1; then cyan \"kubectl:\" && kubectl version --short --client; else yellow \"No kubectl\"; echo; fi;
if command -v kubelet > /dev/null 2>&1; then cyan \"kubelet:\" && kubelet --version; else yellow \"No kubelet\"; echo; fi;
if command -v kubeadm > /dev/null 2>&1; then cyan \"kubeadm:\" && kubeadm version --output short; else yellow \"No kubeadm\"; echo; fi;
if command -v docker > /dev/null 2>&1; then cyan \"docker:\" && docker --version && sudo docker run --rm hello-world 2> /dev/null | grep -o \"Hello from Docker!\"; else yellow \"No Docker\"; echo; fi;
#if command -v helm > /dev/null 2>&1; then cyan \"helm:\" && helm version --short; else yellow \"No helm\"; echo; fi;
if command -v docker-compose > /dev/null 2>&1; then cyan \"Docker Compose:\" && docker-compose --version; fi;
if command -v aws > /dev/null 2>&1; then cyan \"AWS CLI:\" && /usr/local/bin/aws --version; else yellow \"No AWS CLI\"; echo; fi;

echo && grey \"Session Variables\" && echo
echo KUBECONFIG=\${KUBECONFIG}
echo K_APISERVER=\${K_APISERVER}
echo K_PODNETWORK=\${K_PODNETWORK}
echo YAML_FLANNEL=\${YAML_FLANNEL}
echo YAML_CALICO=\${YAML_CALICO}

echo
echo -n \"docker: \" && systemctl status docker | grep --color=never Active
echo -n \"kubelet: \" && systemctl status kubelet | grep --color=never Active

'" + 'alias kreset=\\"sudo su - -c \\\'kubeadm reset -f\\\' \&\& sudo rm -rf /joincluster \&\& sudo rm -rf ${HOME}/.kube \&\& sudo rm -rf /etc/cni/net.d\\"'
